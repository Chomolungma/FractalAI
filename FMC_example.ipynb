{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FAI to solve Atari environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR \n",
    "\n",
    "1. In the notebook toolbar click Kernel -> **Restart & Run all**.\n",
    "2. **Wait a bit while you enjoy how the Agent plays MsPacman**.\n",
    "3. **You should have finished** at least the **first level of MsPacman-v0** using a uniform prior, and about 150 samples per action.\n",
    "4. There is a **video** of the game played **inside** the ***videos* folder** of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractalai.model import RandomDiscreteModel\n",
    "from fractalai.environment import ExternalProcess, ParallelEnvironment, AtariEnvironment\n",
    "from fractalai.fractalmc import FractalMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available games "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of all the Atari games that can be played in Openai Gym using RGB images as observations. Just by changing the game name you can see how the algorithm performs on different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**['AirRaid-v0',  'Alien-v0', 'Amidar-v0', 'Assault-v0', 'Asterix-v0', 'Asteroids-v0', 'Atlantis-v0', 'BankHeist-v0',\n",
    " 'BattleZone-v0', 'BeamRider-v0', 'Berzerk-v0', 'Bowling-v0', 'Boxing-v0', 'Breakout-v0', 'Carnival-v0',\n",
    " 'Centipede-v0', 'ChopperCommand-v0', 'CrazyClimber-v0', 'DemonAttack-v0', 'DoubleDunk-v0', 'ElevatorAction-v0',\n",
    " 'Enduro-v0', 'FishingDerby-v0', 'Freeway-v0', 'Frostbite-v0', 'Gopher-v0', 'Gravitar-v0', 'Hero-v0', 'IceHockey-v0', 'Jamesbond-v0', 'JourneyEscape-v0', 'Kangaroo-v0', 'Krull-v0', 'KungFuMaster-v0', 'MontezumaRevenge-v0', 'MsPacman-v0', 'NameThisGame-v0', 'Phoenix-v0', 'Pitfall-v0', 'Pong-v0', 'Pooyan-v0', 'PrivateEye-v0', 'Qbert-v0',\n",
    " 'Riverraid-v0', 'RoadRunner-v0', 'Robotank-v0', 'Seaquest-v0', 'Skiing-v0', 'Solaris-v0', 'SpaceInvaders-v0',\n",
    " 'StarGunner-v0', 'Tennis-v0', 'TimePilot-v0', 'Tutankham-v0', 'UpNDown-v0', 'Venture-v0', 'VideoPinball-v0',\n",
    " 'WizardOfWor-v0', 'YarsRevenge-v0', 'Zaxxon-v0']**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ram as observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a matrix of pixels, you can also use the ram of the Atari as observations. This will make the calculations a bit lighter, so do not be afraid to check it out!\n",
    "\n",
    "In order to use RAM as observations, add the \"-ram-\" suffix after the game name, and before \"v0\", as shown here:\n",
    "\n",
    "> 'MsPacman-v0' --> 'MsPacman**-ram**-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the parameter choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent relies on four parameters:\n",
    "\n",
    "- **Fixed steps**: It is the number of consecutive times that we will apply an action to the environment when we perturb it choosing an action. Although this parameter actually depends on the Environment, we can use it to manually set the frequency at which the Agent will play. Taking more consecutive actions allows for exploring further in the future at the cost of less reaction time.\n",
    "\n",
    "- **Time Horizon**: This value represents \"how far we need to look into the future when taking an action\". A useful rule of thumb is **Time Horiozon = Nt / Fixed steps**, where **Nt** is the number of frames that it takes the agent to loose one life, (die) since the moment it performs the actions that inevitably lead to its death. This parameters determines the time horizon of the bigger potential well that the Agent should be able to escape.\n",
    "\n",
    "- **Max states**: This is the maximum number of walkers that can be part of the Swarm. This number is related to \"how thick\" we want the resulting causal cone to be. The algorithm will try to use the maximum number of walkers possible. \n",
    "\n",
    "- **Max samples**: This is the maximum number of times that we can make a perturbation when using a Swarm to build a causal cone. It is a superior bound, and the algorithm will try to use less samples to meet the defined **time horizon**. It is a nice way to limit how fast you need to take an action. A reasonable value could be **max walkers** \\* **time horizon** \\* ***N***, being ***N=5*** a number that works well in Atari games, but it depends on the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the [Fractal AI Performance Sheet](https://docs.google.com/spreadsheets/d/1JcNw2L0YL_I2iGZPJ0bNKJshlTaqMuEl5CP2W5zie6M/edit?usp=sharing) to check the parameters we used to run our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Minimal Pacman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tune the Agent to get a decent score on MsPacman using the minimum amount of computational resources possible. We will deliberately set a very small amount of computational resources for calculating an action.\n",
    "\n",
    "Doing that we want to address concerns about edge cases of the theory, by showing how the algorithm performs when the size of the swarm Swarm is very little with respect to the size of the state space.\n",
    "\n",
    "In order to do so, we can give the parameters the following values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"MsPacman-ram-v0\"\n",
    "render = True # It is funnier if the game is displayed on the screen\n",
    "clone_seeds = True  # This will speed things up a bit\n",
    "max_steps = 1e6  # Play until the game is finished.\n",
    "n_repeat_action = 1  # Atari games run at 20 fps, so taking 4 actions per seconds is more \n",
    "reward_limit = 20000\n",
    "render_every = 2\n",
    "dt_mean = 3\n",
    "dt_std = 2\n",
    "min_dt = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAI parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 6000  # Let see how well it can perform using at most 300 samples per step\n",
    "max_walkers = 100 # Let's set a really small number to make everthing faster\n",
    "time_horizon = 30  # 50 frames should be enough to realise you have been eaten by a ghost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters we are aiming for 100 samples per step, saving up to another 200 samples in case the agent runs into trouble. Using such a low number of samples will mean that the performance could vary widely among different runs.\n",
    "\n",
    "In our tests, this agent was capable of finishing the first level most of the times if we set max_states = 15 (150 samples). Using only 100 samples will make it hard for the Agent to find rewards that are far away, so at the end of the first level you will be relying mostly on luck.\n",
    "\n",
    "If you want to get better scores, just increase the values of the parameters accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-3:\n",
      "Process Process-8:\n",
      "Process Process-5:\n",
      "Process Process-6:\n",
      "Process Process-7:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 165, in step\n",
      "    obs, _reward, _end, _info = self._env.step(action)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 75, in step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 165, in step\n",
      "    obs, _reward, _end, _info = self._env.step(action)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 165, in step\n",
      "    obs, _reward, _end, _info = self._env.step(action)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 75, in step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 75, in step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "KeyboardInterrupt\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 165, in step\n",
      "    obs, _reward, _end, _info = self._env.step(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 165, in step\n",
      "    obs, _reward, _end, _info = self._env.step(action)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 75, in step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 810, in _worker\n",
      "    result = getattr(env, name)(*args, **kwargs)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 75, in step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 183, in step\n",
      "    data = new_state, obs, reward, terminal, info\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in step_batch\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 165, in step\n",
      "    obs, _reward, _end, _info = self._env.step(action)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 202, in <listcomp>\n",
      "    for action, state, dt in zip(actions, states, n_repeat_action)]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 75, in step\n",
      "    reward += self.ale.act(action)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/guillem/github/FractalAI/fractalai/environment.py\", line 165, in step\n",
      "    obs, _reward, _end, _info = self._env.step(action)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\", line 75, in step\n",
      "    reward += self.ale.act(action)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\", line 136, in act\n",
      "    return ale_lib.act(self.obj, int(action))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "env = ParallelEnvironment(name=name,env_class=AtariEnvironment,\n",
    "                          blocking=False, n_workers=8, n_repeat_action=n_repeat_action)  # We will play an Atari game\n",
    "model = RandomDiscreteModel(max_wakers=max_walkers,\n",
    "                            n_actions=env.n_actions) # The Agent will take discrete actions at random\n",
    "\n",
    "fmc = FractalMC(model=model, env=env, n_walkers=max_walkers,\n",
    "                reward_limit=reward_limit, render_every=render_every,\n",
    "                time_horizon=time_horizon, dt_mean=dt_mean, dt_std=dt_std, accumulate_rewards=True, min_dt=min_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-951ace067468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_swarm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/FractalAI/fractalai/fractalmc.py\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(self, render, print_swarm)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent_reward\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mi_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_swarm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/fractalmc.py\u001b[0m in \u001b[0;36mrun_swarm\u001b[0;34m(self, state, obs, print_swarm)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_i_simulation\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_condition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_walkers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_i_simulation\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/swarm.py\u001b[0m in \u001b[0;36mstep_walkers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             terms, infos = self._env.step_batch(actions, states=states,\n\u001b[0;32m--> 416\u001b[0;31m                                                 n_repeat_action=self.dt[self._not_frozen])\n\u001b[0m\u001b[1;32m    417\u001b[0m         self.times[self._not_frozen] = (self.times[self._not_frozen] +\n\u001b[1;32m    418\u001b[0m                                         self.dt[self._not_frozen]).astype(np.int32)\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/environment.py\u001b[0m in \u001b[0;36mstep_batch\u001b[0;34m(self, actions, states, n_repeat_action)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                    n_repeat_action: [np.ndarray, int]=None):\n\u001b[1;32m   1006\u001b[0m         return self._batch_env.step_batch(actions=actions, states=states,\n\u001b[0;32m-> 1007\u001b[0;31m                                           n_repeat_action=n_repeat_action)\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeat_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/environment.py\u001b[0m in \u001b[0;36mstep_batch\u001b[0;34m(self, actions, states, n_repeat_action)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             states, observs, rewards, dones, lives = self._make_transitions(actions, states,\n\u001b[0;32m--> 922\u001b[0;31m                                                                             n_repeat_action)\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mobserv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/environment.py\u001b[0m in \u001b[0;36m_make_transitions\u001b[0;34m(self, actions, states, n_repeat_action)\u001b[0m\n\u001b[1;32m    892\u001b[0m                     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m                     \u001b[0m_sts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m                     \u001b[0m_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_sts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mobservs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/environment.py\u001b[0m in \u001b[0;36m_receive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m           \u001b[0mPayload\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;31m# Re-raise exceptions in the main process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_EXCEPTION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fmc.run_agent(render=True, print_swarm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmc.render_game(sleep=1/40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will really appreciate your feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
