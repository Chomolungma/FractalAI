{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractalai.model import RandomDiscreteModel\n",
    "from fractalai.environment import ExternalProcess, ParallelEnvironment, AtariEnvironment\n",
    "from fractalai.swarm_wave import SwarmWave\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SwarmWave?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This algorithhm is made for solving discrete Markov decision processes when a perfect model is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, if you need to generate data to train a neural network, IO is now your bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"MsPacman-ram-v0\"\n",
    "skip_frames = 0  # Number of frames to skip at the beginning\n",
    "dt_mean = 4  # Apply the same action n times in average\n",
    "dt_std = 2 # Repeat same action a variable number of times\n",
    "min_dt = 3 # Minimum number of consecutive steps to be taken\n",
    "n_samples = None  # Maximum number of samples allowed\n",
    "reward_limit = 10000  # Stop the sampling when this score is reached\n",
    "n_walkers = 250  # Maximum witdh of the tree containing al the trajectories\n",
    "render_every = 1  # print statistics every n iterations.\n",
    "balance = 1  # Balance exploration vs exploitation\n",
    "save_data = True # Save the data generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = ParallelEnvironment(name=name,env_class=AtariEnvironment,\n",
    "                          blocking=False, n_workers=8)  # We will play an Atari game\n",
    "model = RandomDiscreteModel(max_wakers=n_walkers,\n",
    "                            n_actions=env.n_actions) # The Agent will take discrete actions at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: MsPacman-ram-v0 | Walkers: 250 | Deaths: 0 | data_size 408\n",
      "Total samples: 393884 Progress: 100.11%\n",
      "Reward: mean 9577.57 | Dispersion: 481.00 | max 10011.00 | min 9530.00 | std 43.67\n",
      "Episode length: mean 1702.92 | Dispersion 50.00 | max 1721.00 | min 1671.00 | std 12.78\n",
      "Dt: mean 4.15 | Dispersion: 5.00 | max 8.00 | min 3.00 | std 1.35\n",
      "Status: Score limit reached.\n",
      "Efficiency 0.40%\n",
      "Generated 1580 Examples | 249.29 samples per example.\n",
      "\n",
      "CPU times: user 7.66 s, sys: 202 ms, total: 7.87 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = SwarmWave(model=model, env=env, n_walkers=n_walkers, reward_limit=reward_limit, \n",
    "              render_every=render_every, balance=balance, save_data=save_data,\n",
    "              dt_mean=dt_mean, dt_std=dt_std, accumulate_rewards=True, min_dt=min_dt)\n",
    "s.run_swarm(print_swarm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.render_game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
